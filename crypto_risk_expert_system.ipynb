{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PpvtgD7Hp6uL",
        "outputId": "2aed4153-2f7e-4092-fae4-697c3d12e71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your CSV decision table file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11542560-89e9-41c9-88bd-9081c8e0e0d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-11542560-89e9-41c9-88bd-9081c8e0e0d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving decision_table_crypto_risk.csv to decision_table_crypto_risk.csv\n",
            "Using uploaded file: decision_table_crypto_risk.csv\n",
            "=== 2.1 Decision classes ===\n",
            "\n",
            "Class: High (count=15)\n",
            "  - Volatility_30d_Level: High:0.867, Medium:0.133\n",
            "  - Momentum_7d_Direction: Up:0.467, Neutral:0.467, Down:0.067\n",
            "  - Liquidity_7d_Level: Low:0.733, Medium:0.267\n",
            "\n",
            "Class: Medium (count=15)\n",
            "  - Volatility_30d_Level: Medium:1.000\n",
            "  - Momentum_7d_Direction: Up:0.667, Neutral:0.333\n",
            "  - Liquidity_7d_Level: Medium:0.600, High:0.400\n",
            "\n",
            "=== 2.2 Indiscernibility relations (IND(B)) ===\n",
            "Using B=['Volatility_30d_Level', 'Momentum_7d_Direction', 'Liquidity_7d_Level'] -> #equivalence classes=11, #pairs=42\n",
            "Equivalence classes with >=2 objects:\n",
            "  * {'Volatility_30d_Level': 'Medium', 'Momentum_7d_Direction': 'Neutral', 'Liquidity_7d_Level': 'High'} -> size=2, decisions=['Medium']\n",
            "  * {'Volatility_30d_Level': 'Medium', 'Momentum_7d_Direction': 'Up', 'Liquidity_7d_Level': 'High'} -> size=4, decisions=['Medium']\n",
            "  * {'Volatility_30d_Level': 'Medium', 'Momentum_7d_Direction': 'Neutral', 'Liquidity_7d_Level': 'Medium'} -> size=3, decisions=['Medium']\n",
            "  * {'Volatility_30d_Level': 'Medium', 'Momentum_7d_Direction': 'Up', 'Liquidity_7d_Level': 'Medium'} -> size=6, decisions=['Medium']\n",
            "  * {'Volatility_30d_Level': 'High', 'Momentum_7d_Direction': 'Up', 'Liquidity_7d_Level': 'Low'} -> size=5, decisions=['High']\n",
            "  * {'Volatility_30d_Level': 'High', 'Momentum_7d_Direction': 'Neutral', 'Liquidity_7d_Level': 'Low'} -> size=4, decisions=['High']\n",
            "  * {'Volatility_30d_Level': 'High', 'Momentum_7d_Direction': 'Neutral', 'Liquidity_7d_Level': 'Medium'} -> size=2, decisions=['High']\n",
            "\n",
            "Compare IND(B) across subsets:\n",
            "  B=['Volatility_30d_Level'] -> #classes=2\n",
            "  B=['Momentum_7d_Direction'] -> #classes=3\n",
            "  B=['Liquidity_7d_Level'] -> #classes=3\n",
            "  B=['Volatility_30d_Level', 'Momentum_7d_Direction'] -> #classes=5\n",
            "  B=['Volatility_30d_Level', 'Liquidity_7d_Level'] -> #classes=5\n",
            "  B=['Momentum_7d_Direction', 'Liquidity_7d_Level'] -> #classes=7\n",
            "  B=['Volatility_30d_Level', 'Momentum_7d_Direction', 'Liquidity_7d_Level'] -> #classes=11\n",
            "\n",
            "=== 2.3 Rough approximations ===\n",
            "Decision=High: |LOW|=15, |UPP|=15, |BND|=0\n",
            "Decision=Medium: |LOW|=15, |UPP|=15, |BND|=0\n",
            "\n",
            "=== 2.4 Quality metrics ===\n",
            "Decision=High: accuracy=1.000, |LOW|/|U|=0.500, |UPP|/|U|=0.500\n",
            "Decision=Medium: accuracy=1.000, |LOW|/|U|=0.500, |UPP|/|U|=0.500\n",
            "\n",
            "=== 2.5 Attribute reduction ===\n",
            "γ(C,D) for full C=['Volatility_30d_Level', 'Momentum_7d_Direction', 'Liquidity_7d_Level']: 1.000\n",
            "Reducts: [['Volatility_30d_Level', 'Liquidity_7d_Level']]\n",
            "CORE: ['Liquidity_7d_Level', 'Volatility_30d_Level']\n",
            "Redundant attribute(s) in this table: ['Momentum_7d_Direction']\n",
            "\n",
            "=== 2.6 Decision rules (from chosen reduct) ===\n",
            "IF {'Volatility_30d_Level': 'High', 'Liquidity_7d_Level': 'Low'} THEN Decision_Risk_Category=High  (support=9)\n",
            "IF {'Volatility_30d_Level': 'High', 'Liquidity_7d_Level': 'Medium'} THEN Decision_Risk_Category=High  (support=4)\n",
            "IF {'Volatility_30d_Level': 'Medium', 'Liquidity_7d_Level': 'High'} THEN Decision_Risk_Category=Medium  (support=6)\n",
            "IF {'Volatility_30d_Level': 'Medium', 'Liquidity_7d_Level': 'Low'} THEN Decision_Risk_Category=High  (support=2)\n",
            "IF {'Volatility_30d_Level': 'Medium', 'Liquidity_7d_Level': 'Medium'} THEN Decision_Risk_Category=Medium  (support=9)\n",
            "\n",
            "Saved rule base to: crypto_risk_rules.txt\n",
            "\n",
            "=== Crypto Risk Expert System (Console) ===\n",
            "Enter attribute values. Type Ctrl+C to exit.\n",
            "\n",
            "Enter Volatility_30d_Level ['High', 'Medium']: high \n",
            "Enter Liquidity_7d_Level ['High', 'Low', 'Medium']: high \n",
            "Choose inference mode ['forward', 'backward']: forward\n",
            "\n",
            "Decision: Unknown (no rule matched)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1789841348.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nSaved rule base to: {RULES_TXT_OUT}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mrun_console_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_reduct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1789841348.py\u001b[0m in \u001b[0;36mrun_console_app\u001b[0;34m(df, rules, reduct)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mfacts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreduct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mfacts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Enter {a}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Choose inference mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"backward\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1789841348.py\u001b[0m in \u001b[0;36mprompt_choice\u001b[0;34m(prompt, options)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mopt_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prompt} {options}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopt_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional, Set\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "\n",
        "# DECISION_TABLE_CSV = \"decision_table_crypto_risk.csv\"  # upload or mount in Colab\n",
        "RULES_TXT_OUT = \"crypto_risk_rules.txt\"\n",
        "\n",
        "COND_ATTRS = [\"Volatility_30d_Level\", \"Momentum_7d_Direction\", \"Liquidity_7d_Level\"]\n",
        "DEC_ATTR = \"Decision_Risk_Category\"\n",
        "\n",
        "# ----------------------------\n",
        "# Upload CSV file\n",
        "# ----------------------------\n",
        "\n",
        "print(\"Please upload your CSV decision table file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    DECISION_TABLE_CSV = uploaded_filename\n",
        "    print(f\"Using uploaded file: {DECISION_TABLE_CSV}\")\n",
        "else:\n",
        "    print(\"No file uploaded. Please upload the 'decision_table_crypto_risk.csv' file manually or ensure it's in the correct path.\")\n",
        "    # Fallback to original name if nothing is uploaded, to potentially raise error later\n",
        "    DECISION_TABLE_CSV = \"decision_table_crypto_risk.csv\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Rough set helpers\n",
        "# ----------------------------\n",
        "\n",
        "def equivalence_classes(df: pd.DataFrame, attrs: List[str]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Returns equivalence classes under IND(attrs).\n",
        "    Each class: key(tuple), objects(Object_IDs), decisions(unique decision values).\n",
        "    \"\"\"\n",
        "    classes = []\n",
        "    groups = df.groupby(attrs, dropna=False, sort=False)\n",
        "    for key, g in groups:\n",
        "        if not isinstance(key, tuple):\n",
        "            key = (key,)\n",
        "        classes.append(\n",
        "            {\n",
        "                \"key\": key,\n",
        "                \"objects\": g[\"Object_ID\"].tolist(),\n",
        "                \"decisions\": sorted(g[DEC_ATTR].unique().tolist()),\n",
        "            }\n",
        "        )\n",
        "    return classes\n",
        "\n",
        "\n",
        "def indiscernible_pairs(classes: List[dict]) -> List[Tuple[str, str]]:\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "    for c in classes:\n",
        "        objs = c[\"objects\"]\n",
        "        if len(objs) < 2:\n",
        "            continue\n",
        "        for i in range(len(objs)):\n",
        "            for j in range(i + 1, len(objs)):\n",
        "                pairs.append((objs[i], objs[j]))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def rough_approximations(df: pd.DataFrame, attrs: List[str], target_decision: str) -> Tuple[Set[str], Set[str], Set[str]]:\n",
        "    \"\"\"\n",
        "    For decision class X:\n",
        "      - lower approximation: union of eq classes fully inside X\n",
        "      - upper approximation: union of eq classes intersecting X\n",
        "      - boundary: upper - lower\n",
        "    \"\"\"\n",
        "    classes = equivalence_classes(df, attrs)\n",
        "\n",
        "    lower: Set[str] = set()\n",
        "    upper: Set[str] = set()\n",
        "\n",
        "    for c in classes:\n",
        "        objs = set(c[\"objects\"])\n",
        "        decisions = set(df.loc[df[\"Object_ID\"].isin(objs), DEC_ATTR].unique())\n",
        "        if decisions == {target_decision}:\n",
        "            lower |= objs\n",
        "            upper |= objs\n",
        "        elif target_decision in decisions:\n",
        "            upper |= objs\n",
        "\n",
        "    boundary = upper - lower\n",
        "    return lower, upper, boundary\n",
        "\n",
        "\n",
        "def positive_region(df: pd.DataFrame, attrs: List[str]) -> Set[str]:\n",
        "    \"\"\"\n",
        "    POS_attrs(D): union of IND(attrs) classes that are decision-consistent\n",
        "    (all objects in class share one decision).\n",
        "    \"\"\"\n",
        "    pos: Set[str] = set()\n",
        "    for c in equivalence_classes(df, attrs):\n",
        "        objs = c[\"objects\"]\n",
        "        if df.loc[df[\"Object_ID\"].isin(objs), DEC_ATTR].nunique() == 1:\n",
        "            pos |= set(objs)\n",
        "    return pos\n",
        "\n",
        "\n",
        "def dependency_degree(df: pd.DataFrame, attrs: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    γ(attrs, D) = |POS_attrs(D)| / |U|\n",
        "    \"\"\"\n",
        "    return len(positive_region(df, attrs)) / len(df)\n",
        "\n",
        "\n",
        "def all_attribute_subsets(attrs: List[str]) -> List[Tuple[str, ...]]:\n",
        "    out: List[Tuple[str, ...]] = []\n",
        "    for r in range(1, len(attrs) + 1):\n",
        "        out.extend(itertools.combinations(attrs, r))\n",
        "    return out\n",
        "\n",
        "\n",
        "def find_reducts(df: pd.DataFrame, cond_attrs: List[str]) -> Tuple[List[Tuple[str, ...]], Tuple[str, ...], Set[str]]:\n",
        "    \"\"\"\n",
        "    Brute-force reduct search (works well for small attribute sets).\n",
        "    - B is a reduct if γ(B,D) == γ(C,D) and minimal.\n",
        "    Returns (reducts, chosen_reduct, core).\n",
        "    \"\"\"\n",
        "    gamma_full = dependency_degree(df, cond_attrs)\n",
        "\n",
        "    candidates = []\n",
        "    for subset in all_attribute_subsets(cond_attrs):\n",
        "        g = dependency_degree(df, list(subset))\n",
        "        if abs(g - gamma_full) < 1e-12:\n",
        "            candidates.append(subset)\n",
        "\n",
        "    reducts: List[Tuple[str, ...]] = []\n",
        "    for s in sorted(candidates, key=len):\n",
        "        # skip if a smaller reduct already contained in s\n",
        "        if any(set(r).issubset(set(s)) for r in reducts):\n",
        "            continue\n",
        "\n",
        "        # ensure minimal\n",
        "        is_minimal = True\n",
        "        for r in range(1, len(s)):\n",
        "            for t in itertools.combinations(s, r):\n",
        "                if t in candidates:\n",
        "                    is_minimal = False\n",
        "                    break\n",
        "            if not is_minimal:\n",
        "                break\n",
        "\n",
        "        if is_minimal:\n",
        "            reducts.append(s)\n",
        "\n",
        "    chosen = sorted(reducts, key=lambda x: (len(x), list(x)))[0] if reducts else tuple(cond_attrs)\n",
        "\n",
        "    core = set(reducts[0]) if reducts else set(cond_attrs)\n",
        "    for r in reducts[1:]:\n",
        "        core &= set(r)\n",
        "\n",
        "    return reducts, chosen, core\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Rule base + inference engine\n",
        "# ----------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Rule:\n",
        "    conditions: Dict[str, str]     # attribute -> value\n",
        "    decision_value: str            # DEC_ATTR value\n",
        "\n",
        "    def matches(self, facts: Dict[str, str]) -> bool:\n",
        "        return all(facts.get(a) == v for a, v in self.conditions.items())\n",
        "\n",
        "\n",
        "def rules_from_table(df: pd.DataFrame, reduct: Tuple[str, ...]) -> List[Rule]:\n",
        "    \"\"\"\n",
        "    Build compact rules from the table using a chosen reduct.\n",
        "    \"\"\"\n",
        "    rules: List[Rule] = []\n",
        "    grouped = df.groupby(list(reduct), dropna=False, sort=True)\n",
        "    for key, g in grouped:\n",
        "        if not isinstance(key, tuple):\n",
        "            key = (key,)\n",
        "\n",
        "        decisions = g[DEC_ATTR].unique()\n",
        "        if len(decisions) != 1:\n",
        "            continue  # skip inconsistent groups (not expected here)\n",
        "\n",
        "        conds = {attr: str(val) for attr, val in zip(reduct, key)}\n",
        "        rules.append(Rule(conditions=conds, decision_value=str(decisions[0])))\n",
        "\n",
        "    return rules\n",
        "\n",
        "\n",
        "def save_rules_txt(rules: List[Rule], filepath: str) -> None:\n",
        "    lines = []\n",
        "    for r in rules:\n",
        "        conds = \" AND \".join([f\"{a}={v}\" for a, v in r.conditions.items()])\n",
        "        lines.append(f\"IF {conds} THEN {DEC_ATTR}={r.decision_value}\")\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "\n",
        "def forward_chain(facts: Dict[str, str], rules: List[Rule]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Data-driven: first matching rule returns a decision.\n",
        "    \"\"\"\n",
        "    for r in rules:\n",
        "        if r.matches(facts):\n",
        "            return r.decision_value\n",
        "    return None\n",
        "\n",
        "\n",
        "def backward_chain(goal: str, facts: Dict[str, str], rules: List[Rule]) -> bool:\n",
        "    \"\"\"\n",
        "    Goal-driven: can we prove DEC_ATTR=goal from facts?\n",
        "    \"\"\"\n",
        "    for r in rules:\n",
        "        if r.decision_value != goal:\n",
        "            continue\n",
        "        if r.matches(facts):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Reporting (Stage 2 outputs)\n",
        "# ----------------------------\n",
        "\n",
        "def print_stage2_analysis(df: pd.DataFrame) -> Tuple[List[Rule], Tuple[str, ...]]:\n",
        "    print(\"=== 2.1 Decision classes ===\")\n",
        "    decisions = sorted(df[DEC_ATTR].unique().tolist())\n",
        "    for d in decisions:\n",
        "        sub = df[df[DEC_ATTR] == d]\n",
        "        print(f\"\\nClass: {d} (count={len(sub)})\")\n",
        "        for a in COND_ATTRS:\n",
        "            dist = sub[a].value_counts(normalize=True)\n",
        "            dist_str = \", \".join([f\"{k}:{v:.3f}\" for k, v in dist.items()])\n",
        "            print(f\"  - {a}: {dist_str}\")\n",
        "\n",
        "    print(\"\\n=== 2.2 Indiscernibility relations (IND(B)) ===\")\n",
        "    full_classes = equivalence_classes(df, COND_ATTRS)\n",
        "    pairs = indiscernible_pairs(full_classes)\n",
        "    print(f\"Using B={COND_ATTRS} -> #equivalence classes={len(full_classes)}, #pairs={len(pairs)}\")\n",
        "    print(\"Equivalence classes with >=2 objects:\")\n",
        "    for c in full_classes:\n",
        "        if len(c[\"objects\"]) < 2:\n",
        "            continue\n",
        "        key_map = {a: v for a, v in zip(COND_ATTRS, c[\"key\"])}\n",
        "        print(f\"  * {key_map} -> size={len(c['objects'])}, decisions={c['decisions']}\")\n",
        "\n",
        "    print(\"\\nCompare IND(B) across subsets:\")\n",
        "    for subset in all_attribute_subsets(COND_ATTRS):\n",
        "        classes = equivalence_classes(df, list(subset))\n",
        "        print(f\"  B={list(subset)} -> #classes={len(classes)}\")\n",
        "\n",
        "    print(\"\\n=== 2.3 Rough approximations ===\")\n",
        "    for d in decisions:\n",
        "        low, upp, bnd = rough_approximations(df, COND_ATTRS, d)\n",
        "        print(f\"Decision={d}: |LOW|={len(low)}, |UPP|={len(upp)}, |BND|={len(bnd)}\")\n",
        "\n",
        "    print(\"\\n=== 2.4 Quality metrics ===\")\n",
        "    U_size = len(df)\n",
        "    for d in decisions:\n",
        "        low, upp, _ = rough_approximations(df, COND_ATTRS, d)\n",
        "        acc = (len(low) / len(upp)) if len(upp) else 0.0\n",
        "        print(f\"Decision={d}: accuracy={acc:.3f}, |LOW|/|U|={len(low)/U_size:.3f}, |UPP|/|U|={len(upp)/U_size:.3f}\")\n",
        "\n",
        "    print(\"\\n=== 2.5 Attribute reduction ===\")\n",
        "    gamma_full = dependency_degree(df, COND_ATTRS)\n",
        "    print(f\"γ(C,D) for full C={COND_ATTRS}: {gamma_full:.3f}\")\n",
        "    reducts, chosen_reduct, core = find_reducts(df, COND_ATTRS)\n",
        "    print(f\"Reducts: {[list(r) for r in reducts]}\")\n",
        "    print(f\"CORE: {sorted(core)}\")\n",
        "\n",
        "    removed = [a for a in COND_ATTRS if a not in chosen_reduct]\n",
        "    if removed:\n",
        "        print(f\"Redundant attribute(s) in this table: {removed}\")\n",
        "\n",
        "    print(\"\\n=== 2.6 Decision rules (from chosen reduct) ===\")\n",
        "    rules = rules_from_table(df, chosen_reduct)\n",
        "    support = df.groupby(list(chosen_reduct)).size().to_dict()\n",
        "    for r in rules:\n",
        "        key = tuple(r.conditions[a] for a in chosen_reduct)\n",
        "        print(f\"IF {r.conditions} THEN {DEC_ATTR}={r.decision_value}  (support={support.get(key, 0)})\")\n",
        "\n",
        "    return rules, chosen_reduct\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Console UI (Stage 3.3)\n",
        "# ----------------------------\n",
        "\n",
        "def prompt_choice(prompt: str, options: List[str]) -> str:\n",
        "    opt_set = {o.lower(): o for o in options}\n",
        "    while True:\n",
        "        raw = input(f\"{prompt} {options}: \").strip().lower()\n",
        "        if raw in opt_set:\n",
        "            return opt_set[raw]\n",
        "        print(\"Invalid choice. Try again.\")\n",
        "\n",
        "\n",
        "def run_console_app(df: pd.DataFrame, rules: List[Rule], reduct: Tuple[str, ...]) -> None:\n",
        "    print(\"\\n=== Crypto Risk Expert System (Console) ===\")\n",
        "    print(\"Enter attribute values. Type Ctrl+C to exit.\\n\")\n",
        "\n",
        "    allowed: Dict[str, List[str]] = {}\n",
        "    for a in reduct:\n",
        "        allowed[a] = sorted({str(v) for v in df[a].unique().tolist()})\n",
        "\n",
        "    while True:\n",
        "        facts: Dict[str, str] = {}\n",
        "        for a in reduct:\n",
        "            facts[a] = prompt_choice(f\"Enter {a}\", allowed[a])\n",
        "\n",
        "        mode = prompt_choice(\"Choose inference mode\", [\"forward\", \"backward\"])\n",
        "        if mode == \"forward\":\n",
        "            decision = forward_chain(facts, rules)\n",
        "            print(f\"\\nDecision: {decision if decision else 'Unknown (no rule matched)'}\\n\")\n",
        "        else:\n",
        "            goal = prompt_choice(f\"Choose goal value for {DEC_ATTR}\", sorted({r.decision_value for r in rules}))\n",
        "            ok = backward_chain(goal, facts, rules)\n",
        "            print(f\"\\nGoal {DEC_ATTR}={goal}: {'PROVED' if ok else 'NOT PROVED'}\\n\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(DECISION_TABLE_CSV)\n",
        "\n",
        "    rules, chosen_reduct = print_stage2_analysis(df)\n",
        "\n",
        "    save_rules_txt(rules, RULES_TXT_OUT)\n",
        "    print(f\"\\nSaved rule base to: {RULES_TXT_OUT}\")\n",
        "\n",
        "    run_console_app(df, rules, chosen_reduct)\n"
      ]
    }
  ]
}